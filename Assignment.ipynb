{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02944396",
      "metadata": {
        "id": "02944396"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9652bc6",
      "metadata": {
        "id": "c9652bc6"
      },
      "outputs": [],
      "source": [
        "# Import \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from Chess_env import *\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "import random\n",
        "# from google.colab import files\n",
        "import pandas as pd\n",
        "from tqdm import trange\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "\n",
        "size_board = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bceca7c",
      "metadata": {
        "id": "0bceca7c"
      },
      "source": [
        "## The Environment\n",
        "\n",
        "You can find the environment in the file Chess_env, which contains the class Chess_env. To define an object, you need to provide the board size considered as input. In our example, size_board=4. \n",
        "Chess_env is composed by the following methods:\n",
        "\n",
        "1. Initialise_game. The method initialises an episode by placing the three pieces considered (Agent's king and queen, enemy's king) in the chess board. The outputs of the method are described below in order.\n",
        "\n",
        "     S $\\;$ A matrix representing the board locations filled with 4 numbers: 0, no piece in that position; 1, location of the \n",
        "     agent's king; 2 location of the queen; 3 location of the enemy king.\n",
        "     \n",
        "     X $\\;$ The features, that is the input to the neural network. See the assignment for more information regarding the            definition of the features adopted. To personalise this, go into the Features method of the class Chess_env() and change        accordingly.\n",
        "     \n",
        "     allowed_a $\\;$ The allowed actions that the agent can make. The agent is moving a king, with a total number of 8                possible actions, and a queen, with a total number of $(board_{size}-1)\\times 8$ actions. The total number of possible actions correspond      to the sum of the two, but not all actions are allowed in a given position (movements to locations outside the borders or      against chess rules). Thus, the variable allowed_a is a vector that is one (zero) for an action that the agent can (can't)      make. Be careful, apply the policy considered on the actions that are allowed only.\n",
        "     \n",
        "\n",
        "2. OneStep. The method performs a one step update of the system. Given as input the action selected by the agent, it updates the chess board by performing that action and the response of the enemy king (which is a random allowed action in the settings considered). The first three outputs are the same as for the Initialise_game method, but the variables are computed for the position reached after the update of the system. The fourth and fifth outputs are:\n",
        "\n",
        "     R $\\;$ The reward. To change this, look at the OneStep method of the class where the rewards are set.\n",
        "     \n",
        "     Done $\\;$ A variable that is 1 if the episode has ended (checkmate or draw).\n",
        "     \n",
        "     \n",
        "3. Features. Given the chessboard position, the method computes the features.\n",
        "\n",
        "This information and a quick analysis of the class should be all you need to get going. The other functions that the class exploits are uncommented and constitute an example on how not to write a python code. You can take a look at them if you want, but it is not necessary.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9593a299",
      "metadata": {
        "id": "9593a299"
      },
      "outputs": [],
      "source": [
        "## INITIALISE THE ENVIRONMENT\n",
        "\n",
        "env=Chess_Env(size_board)\n",
        "\n",
        "N_episodes = 100000\n",
        "max_T = 200 # max iterations of one episode (to avoid endless loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "poBSG0WWLYY7",
      "metadata": {
        "id": "poBSG0WWLYY7"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "VoHukTXELbJl",
      "metadata": {
        "id": "VoHukTXELbJl"
      },
      "outputs": [],
      "source": [
        "# GENERAL AGENT CLASS, used by both SARSA and Q-Learning\n",
        "class GeneralAgent:\n",
        "\n",
        "    # BUILD NEURAL NETWORK with 1 hidden layer with 200 and ReLU activation\n",
        "    def build_model(self):\n",
        "        model = keras.Sequential()\n",
        "        model.add(keras.layers.Dense(200, input_shape=(np.shape(X)), activation='relu'))\n",
        "        model.add(keras.layers.Dense(32, activation='linear'))\n",
        "        return model\n",
        "\n",
        "    # COMPILE MODEL with either Adam or RMSprop\n",
        "    def compile_model(self, opt=\"Adam\"):\n",
        "        if opt == \"Adam\":\n",
        "            self.model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "        elif opt == \"RMSprop\":\n",
        "            self.model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001,\n",
        "                                                                                 rho=0.9,\n",
        "                                                                                 momentum=0.0,\n",
        "                                                                                 epsilon=1e-07,\n",
        "                                                                                 centered=False,\n",
        "                                                                                 name=\"RMSprop\"),\n",
        "                               metrics=[\"accuracy\"])\n",
        "\n",
        "    # EPSILON GREEDY POLICY to select next action\n",
        "    def epsilon_greedy(self, X, allowed_a, epsilon):\n",
        "        allowed_ind,_=np.where(allowed_a==1) # only consider allowed actions\n",
        "\n",
        "        # if the random value is smaller than epsilon take a random action from the allowed actions\n",
        "        rand_value=np.random.uniform(0,1) \n",
        "        if rand_value < epsilon: \n",
        "            a = np.random.choice(allowed_ind) \n",
        "\n",
        "        # else select action with max predicted value \n",
        "        else: \n",
        "            X_reshape = np.reshape(X, (1, self.N_in))           # reshape to fit model\n",
        "            predicted = self.model.predict(X_reshape)           # predict Q-values for X\n",
        "            predicted_allowed = np.take(predicted, allowed_ind) # only take Q-values with allowed indices \n",
        "            a_p = np.argmax(predicted_allowed)                  # select argmax of predicted\n",
        "            a = allowed_ind[a_p]                                # take correct position of action\n",
        "\n",
        "        return a\n",
        "\n",
        "    # LOAD MODEL WEIGHTS\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "    \n",
        "    # SAVE MODEL WEIGHTS\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UCCobONHL6pk",
      "metadata": {
        "id": "UCCobONHL6pk"
      },
      "source": [
        "## Task 3: Deep SARSA\n",
        "Implement SARSA algorithm and produce two plots that show the reward per game and the number of moves per game vs training time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x0HMio4VquNb",
      "metadata": {
        "id": "x0HMio4VquNb"
      },
      "source": [
        "**Agent for SARSA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "hnXppL4OqS2E",
      "metadata": {
        "id": "hnXppL4OqS2E"
      },
      "outputs": [],
      "source": [
        "# SARSA AGENT CLASS inherits from general agent, all functions and variables relevant to SARSA\n",
        "class SarsaAgent(GeneralAgent):\n",
        "    \n",
        "    def __init__(self, N_a, N_in, epsilon=0.2, beta=0.00005, gamma=0.85, eta=0.0035):\n",
        "        # SAVING VARIABLES\n",
        "        # initialize parameters\n",
        "        self.N_a = N_a\n",
        "        self.N_in = N_in\n",
        "\n",
        "        self.epsilon = epsilon     # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
        "        self.beta = beta           # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)\n",
        "        self.gamma = gamma         # THE DISCOUNT FACTOR\n",
        "        self.eta = eta             # THE LEARNING RATE\n",
        "\n",
        "        self.R_save = np.zeros([N_episodes])\n",
        "        self.N_moves_save = np.zeros([N_episodes])\n",
        "\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # TRAIN MODEL with SARSA update rule\n",
        "    def train_model(self, X, a, R, X_next, a_next, Done):        \n",
        "        target = R\n",
        "        X_reshape = np.reshape(X, (1, N_in))\n",
        "        target_f = self.model.predict(X_reshape)\n",
        "\n",
        "        if Done == 0:\n",
        "            target = (R + self.gamma * self.model.predict(np.array([X_next]))[0])[a_next]   # SARSA update rule\n",
        "\n",
        "        target_f[0][a] = target\n",
        "        self.model.fit(X_reshape, target_f, epochs=1, verbose=0)    # train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GrROZg5EMQLI",
      "metadata": {
        "id": "GrROZg5EMQLI"
      },
      "source": [
        "### SARSA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_algorithm(sarsa_agent, N_episodes, max_T, adapted_reward=False):\n",
        "    wandb.config = {\n",
        "        \"N_episodes\": N_episodes,\n",
        "        \"max_T\": max_T\n",
        "    }\n",
        "    t = trange(N_episodes, desc='Bar desc', leave=True)\n",
        "    for e in t:\n",
        "        # epsilon decay\n",
        "        epsilon = sarsa_agent.epsilon / (1 + sarsa_agent.beta * e)\n",
        "        Done = 0  ## SET DONE TO ZERO (BEGINNING OF THE EPISODE)\n",
        "        \n",
        "        S, X, allowed_a = env.Initialise_game()  ## INITIALISE GAME\n",
        "\n",
        "        a = sarsa_agent.epsilon_greedy(X, allowed_a, epsilon)   # get action from greedy\n",
        "        a_next = None # set a_next to None (needed if we solve in one step and train)\n",
        "        time = 1\n",
        "        \n",
        "        while Done == 0:\n",
        "            # take action a\n",
        "            S_next, X_next, allowed_a_next, R, Done = env.OneStep(a, adapted_reward)\n",
        "\n",
        "            # if number of iterations is max nr of iterations, set to Done  \n",
        "            if time == max_T:\n",
        "                Done = 1 \n",
        "\n",
        "            # THE EPISODE HAS NOT ENDED: epsilon greedy to get next action   \n",
        "            if Done == 0:\n",
        "                a_next = sarsa_agent.epsilon_greedy(X_next, allowed_a_next, epsilon) # get action from greedy\n",
        "            \n",
        "            # train model \n",
        "            sarsa_agent.train_model(X, a, R, X_next, a_next, Done)\n",
        "\n",
        "            ## THE EPISODE HAS ENDED, update reward and number of moves\n",
        "            if Done == 1:\n",
        "                sarsa_agent.R_save[e] = np.copy(R)\n",
        "                sarsa_agent.N_moves_save[e] = np.copy(time)\n",
        "                break\n",
        "            \n",
        "            ## wandb logging for plots, remove if not using wandb\n",
        "            wandb.log({\"Epoch\": e})\n",
        "            wandb.log({\"moves_average\": sarsa_agent.N_moves_save[e-100:e].mean() if e > 100 else 0})\n",
        "            wandb.log({\"reward_average\": sarsa_agent.R_save[e-100:e].mean() if e > 100 else 0})\n",
        "            wandb.log({\"moves\": sarsa_agent.N_moves_save[e]})  \n",
        "            wandb.log({\"reward\": sarsa_agent.R_save[e]})  \n",
        "            \n",
        "            # progress bar\n",
        "            t.set_description(f\"Epoch {e}/{N_episodes}\")\n",
        "            t.set_postfix(moves_average = sarsa_agent.N_moves_save[e-100:e].mean() if e > 100 else 0,\n",
        "                          reward_average = sarsa_agent.R_save[e-100:e].mean() if e > 100 else 0)   \n",
        "\n",
        "            # update parameters (state, allowed actions, action)\n",
        "            X = np.copy(X_next)\n",
        "            allowed_a = np.copy(allowed_a_next)\n",
        "            a = np.copy(a_next)\n",
        "            time += 1\n",
        "\n",
        "            # every 1000 iterations save model to avoid losing data\n",
        "            if e % 1000 == 0:\n",
        "                sarsa_agent.save(\"sarsa_chess\")"
      ],
      "metadata": {
        "id": "C7L1lWpIKX51"
      },
      "id": "C7L1lWpIKX51",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ba1f84",
      "metadata": {
        "id": "e6ba1f84"
      },
      "outputs": [],
      "source": [
        "# SARSA\n",
        "# initialize wandb\n",
        "wandb.init(project=\"chess_rl\", name=\"SARSA\", entity=\"ramize\") # change to your username\n",
        "\n",
        "# initialize a game\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in = np.shape(X)[0] \n",
        "sarsa_agent = SarsaAgent(N_a, N_in) #initialize our sarsa agent network\n",
        "sarsa_agent.compile_model() # compile the network\n",
        "\n",
        "# sarsa algorithm\n",
        "sarsa_algorithm(sarsa_agent, N_episodes, max_T)\n",
        "\n",
        "# save model           \n",
        "sarsa_agent.save(\"sarsa\")\n",
        "\n",
        "#transform number of moves and rewards to dataframe with EMA\n",
        "df_n_moves_sarsa = pd.DataFrame({'moves': sarsa_agent.N_moves_save})\n",
        "df_reward_sarsa = pd.DataFrame({'rewards': sarsa_agent.R_save})\n",
        "ema_n_moves_sarsa = df_n_moves_sarsa.ewm(com=5000).mean()\n",
        "ema_reward_sarsa = df_reward_sarsa.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "df_n_moves_sarsa.to_csv(\"csv/df_n_moves_sarsa.csv\")\n",
        "df_reward_sarsa.to_csv(\"csv/df_reward_sarsa.csv\")\n",
        "ema_n_moves_sarsa.to_csv(\"csv/ema_n_moves_sarsa.csv\")\n",
        "ema_reward_sarsa.to_csv(\"csv/ema_reward_sarsa.csv\")\n",
        "\n",
        "sarsa_color = \"blue\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "acFjyqqhqYvH"
      },
      "id": "acFjyqqhqYvH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Moves per Game** "
      ],
      "metadata": {
        "id": "8ZCqF5UpqtVJ"
      },
      "id": "8ZCqF5UpqtVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "sarsa_moves = pd.read_csv('csv/ema_n_moves_sarsa.csv')\n",
        "sarsa_moves = sarsa_moves.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_moves, label=\"SARSA\", color=sarsa_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Moves\")\n",
        "plt.title(\"Number of Moves per Game\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_sarsa.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IX2smfLfqXVr"
      },
      "id": "IX2smfLfqXVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward per Game**"
      ],
      "metadata": {
        "id": "1nv2W3UvrPwh"
      },
      "id": "1nv2W3UvrPwh"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema)\n",
        "sarsa_reward = pd.read_csv('csv/ema_reward_sarsa.csv')\n",
        "sarsa_reward = sarsa_reward.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_reward, label=\"SARSA\", color=sarsa_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Game\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_sarsa.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O1pP67T6rZbe"
      },
      "id": "O1pP67T6rZbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "IuAUCrxTzgUj",
      "metadata": {
        "id": "IuAUCrxTzgUj"
      },
      "source": [
        "## Task 4: Adapt Parameters: Discount Factor and Speed of Decaying Trend"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9d62dd",
      "metadata": {
        "id": "2a9d62dd"
      },
      "source": [
        "We change the discount factor gamma from 0.85 to 0.9, the speed beta from 0.00005 to 0.000005, the decaing trend epsilon from 0.2 to 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762f76c7",
      "metadata": {
        "id": "762f76c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "8abf5006c7a743759005473d45d33af5",
            "7cde9254c8584e739ba2a0c1758742fc",
            "26147ff5a5c0485ba78cd2c3f99c21d4",
            "9d535f15ad5149529ceaa9d765b85238",
            "8cb9ce5602274e8fa18817977efad072",
            "8408a131286a49a6bf54ee4b9368d43d",
            "f2112703fed14b51a2c025afe9b590f9",
            "d8a27fb4425749949ecbde642da9d719"
          ]
        },
        "outputId": "dbdfb998-60aa-41b6-ea54-d53810813572"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:94n68y3z) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8abf5006c7a743759005473d45d33af5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>moves</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>moves_average</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▇▇▇▇▇▇▇█▇█████▇</td></tr><tr><td>reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward_average</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▆▆▅▆▆▆▆▆▆▆▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>171</td></tr><tr><td>moves</td><td>0.0</td></tr><tr><td>moves_average</td><td>9.72</td></tr><tr><td>reward</td><td>0.0</td></tr><tr><td>reward_average</td><td>0.37</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">wise-sky-5</strong>: <a href=\"https://wandb.ai/ramize/chess_rl/runs/94n68y3z\" target=\"_blank\">https://wandb.ai/ramize/chess_rl/runs/94n68y3z</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220324_163224-94n68y3z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:94n68y3z). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220324_163639-9tz1vu6r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ramize/chess_rl/runs/9tz1vu6r\" target=\"_blank\">vocal-rain-6</a></strong> to <a href=\"https://wandb.ai/ramize/chess_rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80859/100000:  81%|████████  | 80859/100000 [7:51:44<2:36:29,  2.04it/s, moves_average=2.84, reward_average=0.97]"
          ]
        }
      ],
      "source": [
        "# SARSA with changed epsilon, beta, gamma\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"SARSA Adapted Parameters\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in = np.shape(X)[0]\n",
        "\n",
        "# adapted epsilon, beta and gamma parameters\n",
        "epsilon=0.1\n",
        "beta=0.000005\n",
        "gamma=0.9\n",
        "\n",
        "sarsa_agent_changed = SarsaAgent(N_a, N_in, epsilon, beta, gamma)\n",
        "sarsa_agent_changed.compile_model()\n",
        "\n",
        "# sarsa algorithm with sarsa agent with changed parameters\n",
        "sarsa_algorithm(sarsa_agent_changed, N_episodes, max_T)\n",
        "\n",
        "#save model         \n",
        "sarsa_agent_changed.save(\"sarsachess_changed\")\n",
        "\n",
        "#transform number of moves and rewards to dataframe with EMA\n",
        "df_n_moves_sarsa_changed = pd.DataFrame({'moves': sarsa_agent_changed.N_moves_save})\n",
        "df_reward_sarsa_changed = pd.DataFrame({'rewards': sarsa_agent_changed.R_save})\n",
        "ema_n_moves_sarsa_changed = df_n_moves_sarsa_changed.ewm(com=5000).mean()\n",
        "ema_reward_sarsa_changed = df_reward_sarsa_changed.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "df_n_moves_sarsa_changed.to_csv(\"csv/df_n_moves_sarsa_changed.csv\")\n",
        "df_reward_sarsa_changed.to_csv(\"csv/df_reward_sarsa_changed.csv\")\n",
        "ema_n_moves_sarsa_changed.to_csv(\"csv/ema_n_moves_sarsa_changed.csv\")\n",
        "ema_reward_sarsa_changed.to_csv(\"csv/ema_reward_sarsa_changed.csv\")\n",
        "sarsa_color_changed = \"lightblue\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "JMceREhj3CjT"
      },
      "id": "JMceREhj3CjT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Moves Per Game: Comparison SARSA vs. SARSA with Adapted Parameters**"
      ],
      "metadata": {
        "id": "SHUIAu3w3EQ9"
      },
      "id": "SHUIAu3w3EQ9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f976c5",
      "metadata": {
        "id": "84f976c5"
      },
      "outputs": [],
      "source": [
        "# load data from csv file (take ema file)\n",
        "sarsa_moves = pd.read_csv('csv/ema_n_moves_sarsa.csv')\n",
        "sarsa_moves = sarsa_moves.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves_changed = pd.read_csv('csv/ema_n_moves_sarsa_changed.csv')\n",
        "sarsa_moves_changed = sarsa_moves_changed.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_moves, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_moves_changed, label=\"SARSA Adapted\", color=sarsa_color_changed)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Moves\")\n",
        "plt.title(\"Number of Moves per Game: SARSA vs SARSA with Adapted Parameters\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_sarsa_parameters_comparison.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward per Game: Comparison SARSA vs. SARSA with Adapted Parameters**"
      ],
      "metadata": {
        "id": "ZbLFjIv-18Rc"
      },
      "id": "ZbLFjIv-18Rc"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema)\n",
        "sarsa_reward = pd.read_csv('csv/ema_reward_sarsa.csv')\n",
        "sarsa_reward = sarsa_reward.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward_changed = pd.read_csv('csv/ema_reward_sarsa_changed.csv')\n",
        "sarsa_reward_changed = sarsa_reward_changed.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_reward, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_reward_changed, label=\"SARSA Adapted\", color=sarsa_color_changed)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Game: SARSA vs SARSA with Adapted Parameters\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_sarsa_parameters_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nluwRPbP189h"
      },
      "id": "nluwRPbP189h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "V-ke9H36LlWd",
      "metadata": {
        "id": "V-ke9H36LlWd"
      },
      "source": [
        "## Task 5: Deep Q-Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1V85R_AvJVI",
      "metadata": {
        "id": "b1V85R_AvJVI"
      },
      "source": [
        "**Deep Q-Learning Agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6vEItTnXvQw9",
      "metadata": {
        "id": "6vEItTnXvQw9"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(GeneralAgent):\n",
        "    def __init__(self, N_a, N_in):\n",
        "        # SAVING VARIABLES\n",
        "        # initialize parameters\n",
        "        self.N_a = N_a\n",
        "        self.N_in = N_in\n",
        "\n",
        "        self.epsilon = 0.2      # STARTING VALUE OF EPSILON FOR THE EPSILON-GREEDY POLICY\n",
        "        self.beta = 0.00005     # THE PARAMETER SETS HOW QUICKLY THE VALUE OF EPSILON IS DECAYING (SEE epsilon_f BELOW)\n",
        "        self.gamma = 0.85       # THE DISCOUNT FACTOR\n",
        "        self.eta = 0.0035       # THE LEARNING RATE\n",
        "\n",
        "        self.R_save = np.zeros([N_episodes]) # \n",
        "        self.N_moves_save = np.zeros([N_episodes])\n",
        "\n",
        "        self.model = self.build_model()\n",
        "\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    # TRAIN MODEL with Q-LEarning update rule\n",
        "    def train_model(self, X, a, R, X_next, Done):\n",
        "        target = R\n",
        "        if Done == 0:\n",
        "            target = (R + self.gamma *\n",
        "                        np.amax(self.model.predict(np.array([X_next]))[0])) #Q-Learning update rule\n",
        "        X_reshape = np.reshape(X, (1, N_in))\n",
        "        target_f = self.model.predict(X_reshape)\n",
        "        target_f[0][a] = target\n",
        "        self.model.fit(X_reshape, target_f, epochs=1, verbose=0)\n",
        "\n",
        "    # EXPERIENCE REPLAY, select batch from memory and train model on this batch\n",
        "    def replay(self): \n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        for X, a, R, X_next, Done in minibatch:\n",
        "            self.train_model(X, a, R, X_next, Done)\n",
        "\n",
        "    # append values to memory (for replay)\n",
        "    def memorize(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-Learning algorithm"
      ],
      "metadata": {
        "id": "R3KCPPln52fr"
      },
      "id": "R3KCPPln52fr"
    },
    {
      "cell_type": "code",
      "source": [
        "def qlearning_algorithm(q_agent, N_episodes, max_T, replay=False, adapted_reward=False):\n",
        "    wandb.config = {\n",
        "        \"N_episodes\": N_episodes,\n",
        "        \"max_T\": max_T\n",
        "    }\n",
        "    t = trange(N_episodes, desc='Bar desc', leave=True)\n",
        "    for e in t:\n",
        "        epsilon = q_agent.epsilon / (1 + q_agent.beta * e)\n",
        "        Done = 0\n",
        "        S, X, allowed_a = env.Initialise_game() # select state X randomly\n",
        "        time = 1\n",
        "        a_next = None\n",
        "\n",
        "        while Done == 0:\n",
        "            a = q_agent.epsilon_greedy(X, allowed_a, epsilon) #choose a from X \n",
        "            S_next, X_next, allowed_a_next, R, Done = env.OneStep(a, adapted_reward) # take action\n",
        "            # when using experience replay, save result and do replay \n",
        "            if replay:\n",
        "                q_agent.memorize(X, a, R, X_next, Done) # append results to memory\n",
        "                if len(q_agent.memory) > q_agent.batch_size and time%100 == 0:\n",
        "                    q_agent.replay() # replay\n",
        "                if time == max_T:\n",
        "                    Done = 1\n",
        "            # else do epsilon greedy and train model\n",
        "            else:\n",
        "                if time == max_T:\n",
        "                    Done = 1\n",
        "                if Done == 0:\n",
        "                    a_next = sarsa_r_agent.epsilon_greedy(X_next, allowed_a_next, epsilon) #make an action\n",
        "                sarsa_r_agent.train_model(X, a, R, X_next, a_next, Done)\n",
        "           \n",
        "            ## IF DONE: save reward and number of moves\n",
        "            if Done == 1:\n",
        "                q_agent.R_save[e] = np.copy(R)\n",
        "                q_agent.N_moves_save[e] = np.copy(time)\n",
        "                break\n",
        "            ## wandb logging for plots, remove if not using wandb\n",
        "            wandb.log({\"Epoch\": e})\n",
        "            wandb.log({\"moves_average\": q_agent.N_moves_save[e-100:e].mean() if e > 100 else 0})\n",
        "            wandb.log({\"reward_average\": q_agent.R_save[e-100:e].mean() if e > 100 else 0})  \n",
        "            # progress bar    \n",
        "            t.set_description(f\"Epoch {e}/{N_episodes}\")\n",
        "            t.set_postfix(moves_average = q_agent.N_moves_save[e-100:e].mean() if e > 100 else 0,\n",
        "                          reward_average = q_agent.R_save[e-100:e].mean() if e > 100 else 0)\n",
        "            \n",
        "            X = X_next\n",
        "            allowed_a = allowed_a_next\n",
        "            time += 1\n",
        "\n",
        "            # every 1000 iterations save model to avoid losing data\n",
        "            if e % 1000 == 0:\n",
        "                q_agent.save(\"qchess\") "
      ],
      "metadata": {
        "id": "fNXtTLzBS2ec"
      },
      "id": "fNXtTLzBS2ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "yMk2YlCkMr6W",
      "metadata": {
        "id": "yMk2YlCkMr6W"
      },
      "source": [
        "**Training Q-Learning algorithm with Experience Replay**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tda1-6DYLLuQ",
      "metadata": {
        "id": "Tda1-6DYLLuQ"
      },
      "outputs": [],
      "source": [
        "# Q_Learning\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"Q-Learning with Replay\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
        "q_agent = DQNAgent(N_a, N_in)\n",
        "\n",
        "# q-learning algorithm with replay=True\n",
        "qlearning_algorithm(q_agent, N_episodes, max_T, True)\n",
        "\n",
        "# save model          \n",
        "q_agent.save(\"qchess_replay\")\n",
        "\n",
        "# for plotting\n",
        "df_n_moves_qlearning_replay = pd.DataFrame({'moves': q_agent.N_moves_save})\n",
        "df_reward_qlearning_replay = pd.DataFrame({'rewards': q_agent.R_save})\n",
        "ema_n_moves_qlearning_replay = df_n_moves_qlearning_replay.ewm(com=5000).mean()\n",
        "ema_reward_qlearning_replay = df_reward_qlearning_replay.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "ema_reward_qlearning_replay.to_csv(\"csv/ema_reward_qlearning_replay.csv\")\n",
        "ema_n_moves_qlearning_replay.to_csv(\"csv/ema_n_moves_qlearning_replay.csv\")\n",
        "df_reward_qlearning_replay.to_csv(\"csv/df_reward_qlearning_replay.csv\")\n",
        "df_n_moves_qlearning_replay.to_csv(\"csv/df_n_moves_qlearning_replay.csv\")\n",
        "qlearning_replay_color = \"lightgreen\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iWVa9Vj9WdvI",
      "metadata": {
        "id": "iWVa9Vj9WdvI"
      },
      "source": [
        "**Training Q-Learning algorithm without Experience Replay**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gg39gL9wQcQ7",
      "metadata": {
        "id": "gg39gL9wQcQ7"
      },
      "outputs": [],
      "source": [
        "# Q_Learning without replay\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"Q-Learning without Replay\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
        "qlearning_agent = DQNAgent(N_a, N_in)\n",
        "\n",
        "# q-learning algorithm with replay=False\n",
        "qlearning_algorithm(qlearning_agent, N_episodes, max_T, False)\n",
        "\n",
        "# save model\n",
        "qlearning_agent.save(\"qlearning_chess\")\n",
        "\n",
        "# for plotting\n",
        "df_n_moves_qlearning = pd.DataFrame({'moves': qlearning_agent.N_moves_save})\n",
        "df_reward_qlearning = pd.DataFrame({'rewards': qlearning_agent.R_save})\n",
        "ema_n_moves_qlearning = df_n_moves_qlearning.ewm(com=5000).mean()\n",
        "ema_reward_qlearning = df_reward_qlearning.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "ema_reward_qlearning.to_csv(\"csv/ema_reward_qlearning.csv\")\n",
        "ema_n_moves_qlearning.to_csv(\"csv/ema_n_moves_qlearning.csv\")\n",
        "df_reward_qlearning.to_csv(\"csv/df_reward_qlearning.csv\")\n",
        "df_n_moves_qlearning.to_csv(\"csv/df_n_moves_qlearning.csv\")\n",
        "qlearning_color = \"green\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "yYt42bcN_TnM"
      },
      "id": "yYt42bcN_TnM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Moves per Game: Q-Learning with and without Experience Replay**"
      ],
      "metadata": {
        "id": "cJwn0rxF_YZe"
      },
      "id": "cJwn0rxF_YZe"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_moves = pd.read_csv(\"csv/ema_n_moves_qlearning.csv\")\n",
        "qlearning_moves = qlearning_moves.iloc[: , 1:]\n",
        "\n",
        "qlearning_moves_replay = pd.read_csv(\"csv/ema_n_moves_qlearning_replay.csv\")\n",
        "qlearning_moves_replay = qlearning_moves_replay.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(qlearning_moves, label=\"Q-Learning without Experience Replay\", color=qlearning_color)\n",
        "plt.plot(qlearning_moves_replay, label=\"Q-Learning with Experience Replay\", color=qlearning_replay_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Moves\")\n",
        "plt.title(\"Number of Moves per Game: QLearning vs QLearning with Experience Replay\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_qlearning_replay_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qOQmsvSm_g-o"
      },
      "id": "qOQmsvSm_g-o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward per Game: Q-Learning with and without Experience Replay**"
      ],
      "metadata": {
        "id": "QkgvTG80Ayoa"
      },
      "id": "QkgvTG80Ayoa"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_reward = pd.read_csv(\"csv/ema_reward_qlearning.csv\")\n",
        "qlearning_reward  = qlearning_reward .iloc[: , 1:]\n",
        "\n",
        "qlearning_reward_replay = pd.read_csv(\"csv/ema_reward_qlearning_replay.csv\")\n",
        "qlearning_reward_replay = qlearning_reward_replay.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(qlearning_reward , label=\"Q-Learning without Experience Replay\", color=qlearning_color)\n",
        "plt.plot(qlearning_reward_replay, label=\"Q-Learning with Experience Replay\", color=qlearning_replay_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Game: QLearning vs QLearning with Experience Replay\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_qlearning_replay_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nOKbvDQ6A4gP"
      },
      "id": "nOKbvDQ6A4gP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SARSA vs Q-Learning**"
      ],
      "metadata": {
        "id": "4pyID2omHKCX"
      },
      "id": "4pyID2omHKCX"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_moves = pd.read_csv(\"csv/ema_n_moves_qlearning.csv\")\n",
        "qlearning_moves = qlearning_moves.iloc[: , 1:]\n",
        "\n",
        "qlearning_moves_replay = pd.read_csv(\"csv/ema_n_moves_qlearning_replay.csv\")\n",
        "qlearning_moves_replay = qlearning_moves_replay.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves = pd.read_csv('csv/ema_n_moves_sarsa.csv')\n",
        "sarsa_moves = sarsa_moves.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves_changed = pd.read_csv('csv/ema_n_moves_sarsa_changed.csv')\n",
        "sarsa_moves_changed = sarsa_moves_changed.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_moves, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_moves_changed, label=\"SARSA Adapted\", color=sarsa_color_changed)\n",
        "plt.plot(qlearning_moves, label=\"Q-Learning without Experience Replay\", color=qlearning_color)\n",
        "plt.plot(qlearning_moves_replay, label=\"Q-Learning with Experience Replay\", color=qlearning_replay_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Moves\")\n",
        "plt.title(\"Number of Moves per Game: SARSA vs Q-Learning\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_qlearning_sarsa_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q6SH46BYHJeC"
      },
      "id": "q6SH46BYHJeC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_reward = pd.read_csv(\"csv/ema_reward_qlearning.csv\")\n",
        "qlearning_reward  = qlearning_reward .iloc[: , 1:]\n",
        "\n",
        "qlearning_reward_replay = pd.read_csv(\"csv/ema_reward_qlearning_replay.csv\")\n",
        "qlearning_reward_replay = qlearning_reward_replay.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward = pd.read_csv('csv/ema_reward_sarsa.csv')\n",
        "sarsa_reward = sarsa_reward.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward_changed = pd.read_csv('csv/ema_reward_sarsa_changed.csv')\n",
        "sarsa_reward_changed = sarsa_reward_changed.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_reward, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_reward_changed, label=\"SARSA Adapted\", color=sarsa_color_changed)\n",
        "plt.plot(qlearning_reward , label=\"Q-Learning without Experience Replay\", color=qlearning_color)\n",
        "plt.plot(qlearning_reward_replay, label=\"Q-Learning with Experience Replay\", color=qlearning_replay_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Game: SARSA vs Q-Learning\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_qlearning_sarsa_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t_xDjDRXHebP"
      },
      "id": "t_xDjDRXHebP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dcoy0Iipxv_u",
      "metadata": {
        "id": "dcoy0Iipxv_u"
      },
      "source": [
        "## Task 6: Change Reward"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We change the reward system in the chess environment. If the action results in a win, we reward 10, if it's a draw we keep the previous reward of 0, and for actions that don't result in Done we punish with a reward of -0.25."
      ],
      "metadata": {
        "id": "OzZLP99Kt_vS"
      },
      "id": "OzZLP99Kt_vS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SARSA with Adapted Reward**"
      ],
      "metadata": {
        "id": "pH9czXsK9_E4"
      },
      "id": "pH9czXsK9_E4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kYL5HLigx2o0",
      "metadata": {
        "id": "kYL5HLigx2o0"
      },
      "outputs": [],
      "source": [
        "# SARSA Adapted Reward\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"SARSA Adapted Reward\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in = np.shape(X)[0]\n",
        "sarsa_r_agent = SarsaAgent(N_a, N_in)\n",
        "\n",
        "sarsa_r_agent.compile_model()\n",
        "#SARSA algorithm with adapted_reward=True\n",
        "sarsa_algorithm(sarsa_r_agent, N_episodes, max_T, True)\n",
        "\n",
        "# save model\n",
        "sarsa_r_agent.save(\"sarsachess_adapted_reward\")\n",
        "\n",
        "# for plotting\n",
        "df_n_moves_sarsa_adapted_reward = pd.DataFrame({'moves': sarsa_r_agent.N_moves_save})\n",
        "df_reward_sarsa_adapted_reward = pd.DataFrame({'rewards': sarsa_r_agent.R_save})\n",
        "ema_n_moves_sarsa_adapted_reward = df_n_moves_sarsa_adapted_reward.ewm(com=5000).mean() \n",
        "ema_reward_sarsa_adapted_reward = df_reward_sarsa_adapted_reward.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "ema_n_moves_sarsa_adapted_reward.to_csv(\"csv/ema_n_moves_sarsa_adapted_reward.csv\")\n",
        "ema_reward_sarsa_adapted_reward.to_csv(\"csv/ema_reward_sarsa_adapted_reward.csv\")\n",
        "df_n_moves_sarsa_adapted_reward.to_csv(\"csv/df_n_moves_sarsa_adapted_reward.csv\")\n",
        "df_reward_sarsa_adapted_reward.to_csv(\"csv/df_reward_sarsa_adapted_reward.csv\")\n",
        "sarsa_adapted_reward_color = \"red\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q-Learning with Adapted Reward**"
      ],
      "metadata": {
        "id": "gvLh645e-Dza"
      },
      "id": "gvLh645e-Dza"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nrr8y4GecSZs",
      "metadata": {
        "id": "Nrr8y4GecSZs"
      },
      "outputs": [],
      "source": [
        "# Q_Learning Adapted Reward\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"QLearning Adapted Reward\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
        "q_agent_adapted = DQNAgent(N_a, N_in)\n",
        "\n",
        "# Q-Learning algorithm with replay=True and adapted_reward=True\n",
        "qlearning_algorithm(q_agent_adapted, N_episodes, max_T, True, True)\n",
        "\n",
        "# save model         \n",
        "q_agent_adapted.save(\"qchess_adapted_reward\")\n",
        "\n",
        "# for plotting\n",
        "df_n_moves_qlearning_replay_adapted_reward = pd.DataFrame({'moves': q_agent_adapted.N_moves_save})\n",
        "df_reward_qlearning_replay_adapted_reward = pd.DataFrame({'rewards': q_agent_adapted.R_save})\n",
        "ema_n_moves_qlearning_replay_adapted_reward = df_n_moves_qlearning_replay_adapted_reward.ewm(com=5000).mean()\n",
        "ema_reward_qlearning_replay_adapted_reward = df_reward_qlearning_replay_adapted_reward.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "df_reward_qlearning_replay_adapted_reward.to_csv(\"csv/df_reward_qlearning_replay_adapted_reward.csv\")\n",
        "df_n_moves_qlearning_replay_adapted_reward.to_csv(\"csv/df_n_moves_qlearning_replay_adapted_reward.csv\")\n",
        "ema_n_moves_qlearning_replay_adapted_reward.to_csv(\"csv/ema_n_moves_qlearning_replay_adapted_reward.csv\")\n",
        "ema_reward_qlearning_replay_adapted_reward.to_csv(\"csv/ema_reward_qlearning_replay_adapted_reward.csv\")\n",
        "qlearning_replay_adapted_color = \"orange\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "URNAQ9iWDX_J"
      },
      "id": "URNAQ9iWDX_J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Moves per Game**"
      ],
      "metadata": {
        "id": "YdHNSAH1DbqA"
      },
      "id": "YdHNSAH1DbqA"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_moves_replay_adapted_reward = pd.read_csv(\"csv/ema_n_moves_qlearning_replay_adapted_reward.csv\")\n",
        "qlearning_moves_replay_adapted_reward = qlearning_moves_replay_adapted_reward.iloc[: , 1:]\n",
        "\n",
        "qlearning_moves_replay = pd.read_csv(\"csv/ema_n_moves_qlearning_replay.csv\")\n",
        "qlearning_moves_replay = qlearning_moves_replay.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves = pd.read_csv('csv/ema_n_moves_sarsa.csv')\n",
        "sarsa_moves = sarsa_moves.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves_adapted_reward = pd.read_csv('csv/ema_n_moves_sarsa_adapted_reward.csv')\n",
        "sarsa_moves_adapted_reward = sarsa_moves_adapted_reward.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_moves, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_moves_adapted_reward, label=\"SARSA with Adapted Rewards\", color=sarsa_adapted_reward_color)\n",
        "plt.plot(qlearning_moves_replay, label=\"Q-Learning\", color=qlearning_replay_color)\n",
        "plt.plot(qlearning_moves_replay_adapted_reward, label=\"Q-Learning with Adapted Rewards\", color=qlearning_replay_adapted_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Moves\")\n",
        "plt.title(\"Number of Moves per Game: Changed Reward Administration\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_adapted_reward_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kHt_3CskBsdi"
      },
      "id": "kHt_3CskBsdi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward per Game**"
      ],
      "metadata": {
        "id": "bYC-dUzIDl4J"
      },
      "id": "bYC-dUzIDl4J"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_reward_replay_adapted_reward = pd.read_csv(\"csv/ema_reward_qlearning_replay_adapted_reward.csv\")\n",
        "qlearning_reward_replay_adapted_reward  = qlearning_reward_replay_adapted_reward .iloc[: , 1:]\n",
        "\n",
        "qlearning_reward_replay = pd.read_csv(\"csv/ema_reward_qlearning_replay.csv\")\n",
        "qlearning_reward_replay = qlearning_reward_replay.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward = pd.read_csv('csv/ema_reward_sarsa.csv')\n",
        "sarsa_reward = sarsa_reward.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward_adapted_reward = pd.read_csv('csv/ema_reward_sarsa_adapted_reward.csv')\n",
        "sarsa_reward_adapted_reward = sarsa_reward_adapted_reward.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_reward, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_reward_adapted_reward, label=\"SARSA with Adapted Rewards\", color=sarsa_adapted_reward_color)\n",
        "plt.plot(qlearning_reward_replay, label=\"Q-Learning\", color=qlearning_replay_color)\n",
        "plt.plot(qlearning_reward_replay_adapted_reward , label=\"Q-Learning with Adapted Rewards\", color=qlearning_replay_adapted_color)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Game: Changed Reward Administration\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_adapted_reward_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "duMFOMCTDp-6"
      },
      "id": "duMFOMCTDp-6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Fix Exploding Gradient"
      ],
      "metadata": {
        "id": "TcNucP_8r684"
      },
      "id": "TcNucP_8r684"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fix the exploding gradient problem, we use the RMSprop instead of Adam as an optimizer, i.e. we compile the model with the RMSprop optimizer."
      ],
      "metadata": {
        "id": "UWx7_qhLucGT"
      },
      "id": "UWx7_qhLucGT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sarsa with RMSprop as optimizer**"
      ],
      "metadata": {
        "id": "NiiM09qgrczX"
      },
      "id": "NiiM09qgrczX"
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"SARSA RMS\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in = np.shape(X)[0]\n",
        "sarsa_agent_rms = SarsaAgent(N_a, N_in)\n",
        "sarsa_agent_rms.compile_model(\"RMSprop\") #compile model with RMSprop\n",
        "\n",
        "sarsa_algorithm(sarsa_agent_rms, N_episodes, max_T)\n",
        "\n",
        "sarsa_agent_rms.save(\"sarsachess_rms\")\n",
        "\n",
        "# for plotting\n",
        "df_n_moves_sarsa_rms = pd.DataFrame({'moves': sarsa_agent_rms.N_moves_save})\n",
        "df_reward_sarsa_rms = pd.DataFrame({'rewards': sarsa_agent_rms.R_save})\n",
        "ema_n_moves_sarsa_rms = df_n_moves_sarsa_rms.ewm(com=5000).mean()\n",
        "ema_reward_sarsa_rms = df_reward_sarsa_rms.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "ema_reward_sarsa_rms.to_csv(\"csv/ema_reward_sarsa_rms.csv\")\n",
        "ema_n_moves_sarsa_rms.to_csv(\"csv/ema_n_moves_sarsa_rms.csv\")\n",
        "df_n_moves_sarsa_rms.to_csv(\"csv/df_n_moves_sarsa_rms.csv\")\n",
        "df_reward_sarsa_rms.to_csv(\"csv/df_reward_sarsa_rms.csv\")\n",
        "sarsa_color_rms = \"pink\""
      ],
      "metadata": {
        "id": "Z18L1mFgsAtB"
      },
      "id": "Z18L1mFgsAtB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q-Learning with RMSprop as optimizer**"
      ],
      "metadata": {
        "id": "6AS-4fRxrZR9"
      },
      "id": "6AS-4fRxrZR9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Q_Learning\n",
        "\n",
        "wandb.init(project=\"chess_rl\", name=\"QLearning RMS\", entity=\"ramize\")\n",
        "\n",
        "S,X,allowed_a=env.Initialise_game()\n",
        "\n",
        "N_a=np.shape(allowed_a)[0]   # TOTAL NUMBER OF POSSIBLE ACTIONS\n",
        "N_in=np.shape(X)[0]    ## INPUT SIZE\n",
        "q_agent_rms = DQNAgent(N_a, N_in)\n",
        "q_agent_rms.compile_model(\"RMSprop\") #compile model with RMSprop\n",
        "\n",
        "qlearning_algorithm(q_agent_rms, N_episodes, max_T)\n",
        "\n",
        "q_agent_rms.save(\"qchess_rms\")\n",
        "\n",
        "# for plotting\n",
        "df_n_moves_qlearning_replay_rms = pd.DataFrame({'moves': q_agent_rms.N_moves_save})\n",
        "df_reward_qlearning_replay_rms = pd.DataFrame({'rewards': q_agent_rms.R_save})\n",
        "ema_n_moves_qlearning_replay_rms = df_n_moves_qlearning_replay_rms.ewm(com=5000).mean()\n",
        "ema_reward_qlearning_replay_rms = df_reward_qlearning_replay_rms.ewm(com=5000).mean()\n",
        "# save df as csv\n",
        "ema_reward_qlearning_replay_rms.to_csv(\"csv/ema_reward_qlearning_replay_rms.csv\")\n",
        "ema_n_moves_qlearning_replay_rms.to_csv(\"csv/ema_n_moves_qlearning_replay_rms.csv\")\n",
        "df_n_moves_qlearning_replay_rms.to_csv(\"csv/df_n_moves_qlearning_replay_rms.csv\")\n",
        "df_reward_qlearning_replay_rms.to_csv(\"csv/df_reward_qlearning_replay_rms.csv\")\n",
        "qlearning_color_rms = \"yellow\""
      ],
      "metadata": {
        "id": "o8hGkDFErUKH"
      },
      "id": "o8hGkDFErUKH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots"
      ],
      "metadata": {
        "id": "HYh1V7EgOYRx"
      },
      "id": "HYh1V7EgOYRx"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_moves_replay_rms = pd.read_csv(\"csv/ema_n_moves_qlearning_replay_rms.csv\")\n",
        "qlearning_moves_replay_rms = qlearning_moves_replay_rms.iloc[: , 1:]\n",
        "\n",
        "qlearning_moves_replay = pd.read_csv(\"csv/ema_n_moves_qlearning_replay.csv\")\n",
        "qlearning_moves_replay = qlearning_moves_replay.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves = pd.read_csv('csv/ema_n_moves_sarsa.csv')\n",
        "sarsa_moves = sarsa_moves.iloc[: , 1:]\n",
        "\n",
        "sarsa_moves_rms = pd.read_csv('csv/ema_n_moves_sarsa_rms.csv')\n",
        "sarsa_moves_rms = sarsa_moves_rms.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_moves, label=\"SARSA with Adam\", color=sarsa_color)\n",
        "plt.plot(sarsa_moves_rms, label=\"SARSA with RMS\", color=sarsa_color_rms)\n",
        "plt.plot(qlearning_moves_replay, label=\"Q-Learning with Adam\", color=qlearning_replay_color)\n",
        "plt.plot(qlearning_moves_replay_rms, label=\"Q-Learning with RMS\", color=qlearning_color_rms)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Moves\")\n",
        "plt.title(\"Number of Moves per Game: Different Optimization Adam vs RMSProp\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_optimization_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zw3A54FKOb5j"
      },
      "id": "Zw3A54FKOb5j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from csv file (take ema file)\n",
        "qlearning_reward_replay_rms = pd.read_csv(\"csv/ema_reward_qlearning_replay_rms.csv\")\n",
        "qlearning_reward_replay_rms  = qlearning_reward_replay_rms .iloc[: , 1:]\n",
        "\n",
        "qlearning_reward_replay = pd.read_csv(\"csv/ema_reward_qlearning_replay.csv\")\n",
        "qlearning_reward_replay = qlearning_reward_replay.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward = pd.read_csv('csv/ema_reward_sarsa.csv')\n",
        "sarsa_reward = sarsa_reward.iloc[: , 1:]\n",
        "\n",
        "sarsa_reward_rms = pd.read_csv('csv/ema_reward_sarsa_rms.csv')\n",
        "sarsa_reward_rms = sarsa_reward_rms.iloc[: , 1:]\n",
        "\n",
        "# plot\n",
        "plt.plot(sarsa_reward, label=\"SARSA with Adam\", color=sarsa_color)\n",
        "plt.plot(sarsa_reward_rms, label=\"SARSA with RMS\", color=sarsa_color_rms)\n",
        "plt.plot(qlearning_reward_replay, label=\"Q-Learning with Adam\", color=qlearning_replay_color)\n",
        "plt.plot(qlearning_reward_replay_rms , label=\"Q-Learning with RMS\", color=qlearning_color_rms)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Reward per Game: Different Optimization Adam vs RMSProp\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_adapted_reward_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2kqoF0L4Obtt"
      },
      "id": "2kqoF0L4Obtt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "iDQyBs2owUCt",
      "metadata": {
        "id": "iDQyBs2owUCt"
      },
      "source": [
        "## Plots: Overall Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kVdmps6WAXQv",
      "metadata": {
        "id": "kVdmps6WAXQv"
      },
      "source": [
        "### Number of Moves per Game vs Training Time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sarsa_moves, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_moves_changed, label=\"SARSA Adapted Parameters\", color=sarsa_color_changed)\n",
        "plt.plot(sarsa_moves_adapted_reward, label=\"SARSA with Adapted Rewards\", color=sarsa_adapted_reward_color)\n",
        "plt.plot(sarsa_moves_rms, label=\"SARSA with RMS\", color=sarsa_color_rms)\n",
        "plt.plot(qlearning_moves_replay, label=\"Q-Learning with Experience Replay\", color=qlearning_replay_color)\n",
        "plt.plot(qlearning_moves, label=\"Q-Learning without Experience Replay\", color=qlearning_color)\n",
        "plt.plot(qlearning_moves_replay_adapted_reward, label=\"Q-Learning with Adapted Rewards\", color=qlearning_replay_adapted_color)\n",
        "plt.plot(qlearning_moves_replay_rms, label=\"Q-Learning with RMS\", color=qlearning_color_rms)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Comparison: Number of Moves per Game\")\n",
        "plt.legend()\n",
        "plt.savefig(\"moves_comparison.pdf\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tsrrzZI_IS-A"
      },
      "id": "tsrrzZI_IS-A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward per Game vs Training Time"
      ],
      "metadata": {
        "id": "UeuDl_rzITSt"
      },
      "id": "UeuDl_rzITSt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vf2mC1ur08iG",
      "metadata": {
        "id": "vf2mC1ur08iG"
      },
      "outputs": [],
      "source": [
        "plt.plot(sarsa_reward, label=\"SARSA\", color=sarsa_color)\n",
        "plt.plot(sarsa_reward_changed, label=\"SARSA Adapted Parameters\", color=sarsa_color_changed)\n",
        "plt.plot(sarsa_reward_adapted_reward, label=\"SARSA with Adapted Rewards\", color=sarsa_adapted_reward_color)\n",
        "plt.plot(sarsa_reward_rms, label=\"SARSA with RMS\", color=sarsa_color_rms)\n",
        "plt.plot(qlearning_reward_replay, label=\"Q-Learning with Experience Replay\", color=qlearning_replay_color)\n",
        "plt.plot(qlearning_reward , label=\"Q-Learning without Experience Replay\", color=qlearning_color)\n",
        "plt.plot(qlearning_reward_replay_adapted_reward , label=\"Q-Learning with Adapted Rewards\", color=qlearning_replay_adapted_color)\n",
        "plt.plot(qlearning_reward_replay_rms , label=\"Q-Learning with RMS\", color=qlearning_color_rms)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.title(\"Comparison: Reward per Game\")\n",
        "plt.legend()\n",
        "plt.savefig(\"reward_comparison.pdf\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8abf5006c7a743759005473d45d33af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cde9254c8584e739ba2a0c1758742fc",
              "IPY_MODEL_26147ff5a5c0485ba78cd2c3f99c21d4"
            ],
            "layout": "IPY_MODEL_9d535f15ad5149529ceaa9d765b85238"
          }
        },
        "7cde9254c8584e739ba2a0c1758742fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb9ce5602274e8fa18817977efad072",
            "placeholder": "​",
            "style": "IPY_MODEL_8408a131286a49a6bf54ee4b9368d43d",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "26147ff5a5c0485ba78cd2c3f99c21d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2112703fed14b51a2c025afe9b590f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8a27fb4425749949ecbde642da9d719",
            "value": 1
          }
        },
        "9d535f15ad5149529ceaa9d765b85238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb9ce5602274e8fa18817977efad072": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8408a131286a49a6bf54ee4b9368d43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2112703fed14b51a2c025afe9b590f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a27fb4425749949ecbde642da9d719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}